[
  {
    "title": "US1.1: Environment & Tooling Setup",
    "body": "**As a** developer,\n**I want** to provision and verify my local development environment (Python 3.9+, Ollama, AINative API key, React/Vue tooling),\n**So that** I can begin implementing code without configuration roadblocks.\n\n**Acceptance Criteria:**\n- Ollama is running locally (`ollama serve vicuna-13b`).\n- `AINATIVE_API_KEY` and `AINATIVE_BASE_URL` are set in `.env`.\n- `backend/` has a functioning virtual environment; `pip install -r requirements.txt` passes without errors.\n- `frontend/` (Create React App or Vue CLI) builds and runs without errors.\n- Folder structure exists: `backend/` and `frontend/` with initial files.\n",
    "labels": ["Epic 1", "Sprint 1"]
  },
  {
    "title": "US1.2: Data Model & Database Setup",
    "body": "**As a** developer,\n**I want** to define and migrate the relational schema (`projects`, `generation_steps`, `logs`, `memory_records`),\n**So that** I can persist app generation metadata, tool-execution steps, logs, and memory references.\n\n**Acceptance Criteria:**\n- PostgreSQL (or SQLite) instance is reachable and configured.\n- SQL migrations (or SQLAlchemy `Base.metadata.create_all`) create the four tables exactly as specified in the data model.\n- Each table’s columns, data types, defaults, and constraints match the PRD’s definitions.\n- Verifying via `psql` or a database GUI shows the tables exist and are empty.\n",
    "labels": ["Epic 1", "Sprint 1"]
  },
  {
    "title": "US1.3: AINative Tool Wrappers",
    "body": "**As a** developer,\n**I want** to implement Python wrappers around AINative’s endpoints (`code-generation/create`, `code-generation/refactor`, `agent/memory`, `agent/memory/search`),\n**So that** I can call AINative programmatically under a consistent interface.\n\n**Acceptance Criteria:**\n- Classes `CodeGenCreateTool`, `CodeGenRefactorTool`, `MemoryStoreTool`, and `MemorySearchTool` exist under `backend/tools/`.\n- Each wrapper sends a POST to the correct AINative path with the header `Authorization: Bearer {API_KEY}`.\n- When instantiated and called with dummy data, each returns either a valid JSON response (if AINative is reachable) or a structured error object: `{ \"error\": true, \"message\": \"...\" }`.\n- No syntactic or import errors occur when importing these classes.\n",
    "labels": ["Epic 1", "Sprint 1"]
  },
  {
    "title": "US1.4: Basic File-Writer Utility",
    "body": "****As a** developer,\n**I want** a small utility (`make_dirs`, `write_file`) to write code strings to a given file path under a base folder,\n**So that** I can assemble the generated project on disk.\n\n**Acceptance Criteria:**\n- File `backend/utils/file_writer.py` exists with two functions:\n  ```python\n  def make_dirs(path: str):\n      os.makedirs(path, exist_ok=True)\n\n  def write_file(path: str, content: str):\n      with open(path, \"w\", encoding=\"utf-8\") as f:\n          f.write(content)\n  ```\n- Running a quick test (`write_file(\"temp/foo/bar.txt\", \"hello\")`) creates the directories `temp/foo/` and writes `bar.txt` with content \"hello\".\n- No uncaught exceptions when writing files.\n",
    "labels": ["Epic 1", "Sprint 1"]
  },
  {
    "title": "US1.5: Backend Endpoint Skeleton & LangChain Integration",
    "body": "**As a** developer,\n**I want** to scaffold two FastAPI endpoints—`POST /generate-app` and `GET /recall-last-app`—and wire in a minimal LangChain + Ollama \"plan\" call,\n**So that** the backend is ready to receive app specs and plan out steps.\n\n**Acceptance Criteria:**\n- `backend/main.py` defines the endpoints:\n  ```python\n  @app.post(\"/generate-app\")\n  async def generate_app(spec: dict):\n      pass  # placeholder\n\n  @app.get(\"/recall-last-app\")\n  async def recall_last_app():\n      pass  # placeholder\n  ```\n- In `generate_app`: validate that `spec` contains all required fields: `project_name`, `description`, `features`, `tech_stack`, `styling`.\n- Instantiate Ollama via:\n  ```python\n  ollama = Ollama(base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"), model=\"vicuna-13b\", temperature=0.2, max_tokens=1024)\n  ```\n- Call `plan_text = ollama.generate(\"<placeholder prompt>\")`, parse with `json.loads(plan_text)`, handling JSON decode errors by returning HTTP 500.\n- Insert a new row into `projects` (status=`IN_PROGRESS`, capturing `agent_id`).\n- `generate_app` returns `{ \"status\": \"in_progress\", \"project_id\": \"<uuid>\" }`.\n- In `recall_last_app`: call `MemorySearchTool()._call(query=\"latest app spec\")`, return 404 if no results, else parse and return JSON with `project_name`, `description`, `features`, `tech_stack`, `styling`.\n- `curl` or Postman tests confirm endpoints return the expected stubs and HTTP codes.\n",
    "labels": ["Epic 1", "Sprint 1"]
  },
  {
    "title": "US1.6: Step Insertion & Status Logging",
    "body": "****As a** developer,\n**I want** to take the JSON plan from Ollama (an array of steps) and insert each step into the `generation_steps` table (status=`PENDING`),\n**So that** I can later track execution status per step.\n\n**Acceptance Criteria:**\n- After parsing `steps = json.loads(plan_text)`, for each `(i, step_obj)` in `steps`, insert into `generation_steps`:\n  - `project_id`: the new project’s UUID\n  - `sequence_order`: `i + 1`\n  - `tool_name`: `step_obj[\"tool\"]`\n  - `input_payload`: JSONB of `step_obj[\"input\"]`\n  - `status`: `'PENDING'`\n- If any insert fails, the transaction rolls back and HTTP 500 is returned.\n- Querying `SELECT * FROM generation_steps WHERE project_id = <uuid> ORDER BY sequence_order` returns exactly one row per step.\n",
    "labels": ["Epic 1", "Sprint 1"]
  },
  {
    "title": "US1.7: Execution Loop & File Assembly",
    "body": "****As a** developer,\n**I want** to iterate over all `PENDING` `generation_steps`, call the corresponding AINative wrapper, update each row's `status` to `SUCCESS`/`FAILED` and store `output_payload`, then write code (if any) to disk under `temp_projects/<project_id>/<file_path>`,\n**So that** the full project folder (frontend + backend) is assembled.\n\n**Acceptance Criteria:**\n- The backend code queries all steps in `sequence_order` where `status = 'PENDING'`.\n- For each step:\n  1. Identify `tool_name` and `input_payload`.\n  2. Call `tools[tool_name]._call(**input_payload)`.\n  3. On return (`obs`), update the corresponding `generation_steps` row: set `output_payload = obs` and:\n     - `status = 'SUCCESS'` if `obs` does not contain `\"error\"`.\n     - Otherwise, `status = 'FAILED'` and record the error message.\n  4. If `obs` contains both `\"file_path\"` and `\"code\"`, compute `full_path = os.path.join(base_dir, obs[\"file_path\"])`, call `make_dirs(os.path.dirname(full_path))`, then `write_file(full_path, obs[\"code\"])`.\n  5. Insert a `logs` entry for each tool call outcome (`\"Step 3 (codegen_create) succeeded: wrote backend/app/models.py\"` or failure details).\n- If a step fails, do not abort the loop, but record it and continue executing subsequent steps.\n- After completion, confirm that at least one file was written in `temp_projects/<project_id>/...`.\n",
    "status": "completed",
    "labels": ["Epic 1", "Sprint 1"]
  },
  {
    "title": "US1.8: ZIP Packaging & Download URL",
    "body": "****As a** developer,\n**I want** to zip the entire `temp_projects/<project_id>/` folder into `downloads/<project_name>.zip` and update `projects.download_url = \"/downloads/<project_name>.zip\"`, `projects.status = \"SUCCESS\"`,\n**So that** the browser can offer a `Download ZIP` link.\n\n**Acceptance Criteria:**\n- After the execution loop, code runs:\n  ```python\n  zip_path = f\"downloads/{project_name}.zip\"\n  with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n      for root, _, files in os.walk(base_dir):\n          for fname in files:\n              abs_path = os.path.join(root, fname)\n              rel_path = os.path.relpath(abs_path, base_dir)\n              zipf.write(abs_path, arcname=os.path.join(project_name, rel_path))\n  ```\n- A new file `downloads/<project_name>.zip` exists.\n- `projects` row is updated with:\n  ```sql\n  UPDATE projects\n    SET download_url = '/downloads/<project_name>.zip',\n        status = 'SUCCESS',\n        updated_at = NOW()'\n  WHERE project_id = <uuid>;\n  ```\n- A `GET http://localhost:8000/downloads/<project_name>.zip` returns the ZIP without permission errors.\n",
    "status": "completed",
    "labels": ["Epic 1", "Sprint 1"]
  },
  {
    "title": "US1.9: Status Logging & Streaming",
    "body": "****As a** user,\n**I want** to see real-time logs (e.g., “Planning…”, “Generated file: backend/app/models.py”, “Packaging complete”) in the UI as the backend works,\n**So that** I know the generation progress and can diagnose failures.\n\n**Acceptance Criteria:**\n- After each major action (parsing plan, calling a tool, writing a file, creating the ZIP), backend inserts a row into `logs(project_id, log_text)`.\n- The `/generate-app` response JSON includes an array field `\"logs\": [<all log_text so far>]`.\n- The frontend’s StatusPanel component appends each new log entry to a scrolling list in real time.\n- If a tool call fails, a log entry such as `\"Step 3 (codegen_create) failed: <error message>\"` appears and is rendered in red or flagged.\n",
    "labels": ["Epic 1", "Sprint 1"]
  },
  {
    "title": "US1.10: Recall Last App End-to-End",
    "body": "****As a** user,\n**I want** to click “Recall Last App” in the browser,\n**So that** the form fields are pre-populated with my most recent app spec.\n\n**Acceptance Criteria:**\n- Frontend sends `GET /recall-last-app`.\n- Backend calls `MemorySearchTool()._call(query=\"latest app spec\")`; if no results, returns HTTP 404 `{ \"error\": \"No previous spec found.\" }`.\n- If a result is found, backend returns JSON with keys: `project_name`, `description`, `features`, `tech_stack`, `styling`.\n- The frontend receives the JSON and populates the form fields accordingly (or logs the spec if direct pre-fill is not yet implemented).\n- If no memory exists, the UI displays an error banner: “No previous spec found.”\n",
    "labels": ["Epic 1", "Sprint 1"]
  },
  {
    "title": "US2.1: Add “Vue + Node.js + MongoDB” Support",
    "body": "****As a** user,\n**I want** to select “Vue + Node.js + MongoDB” from the Tech Stack dropdown,\n**So that** I can get a Vue frontend and Node/Express + Mongoose backend scaffold.\n\n**Acceptance Criteria:**\n1. AINative templates exist and are registered for:\n   - `vue-component`\n   - `express-route`\n   - `mongoose-model`\n   - Manifest files: `frontend/package.json`, `backend/package.json`\n2. `app_generation_plan.txt` is updated to detect `tech_stack == \"Vue + Node.js + MongoDB\"` and produce a valid JSON plan including steps such as:\n   ```json\n   {\n     \"tool\": \"codegen_create\",\n     \"input\": { \"template\": \"mongoose-model\", \"file_path\": \"backend/models/User.js\", \"variables\": { ... } }\n   },\n   {\n     \"tool\": \"codegen_create\",\n     \"input\": { \"template\": \"express-route\", \"file_path\": \"backend/routes/auth.js\", \"variables\": { ... } }\n   },\n   {\n     \"tool\": \"codegen_create\",\n     \"input\": { \"template\": \"vue-component\", \"file_path\": \"frontend/src/components/Login.vue\", \"variables\": { ... } }\n   }\n   ```\n3. Frontend `SpecForm.jsx` includes “Vue + Node.js + MongoDB” as an option.\n4. Running `POST /generate-app` with that stack produces a ZIP containing:\n   - `frontend/src/components/*.vue`\n   - `backend/models/*.js` and `backend/routes/*.js`\n   - `frontend/package.json` with dependencies `vue`, `vue-router`, `axios`\n   - `backend/package.json` with dependencies `express`, `mongoose`, etc.\n5. Unzipping and running `npm install`/`npm start` in the frontend yields no errors; similarly, `npm install`/`node backend/server.js` in the backend yields no server startup errors.\n",
    "labels": ["Epic 2", "Sprint 2"]
  },
  {
    "title": "US2.2: Add “Next.js + Django + MySQL” Support",
    "body": "****As a** user,\n**I want** to select “Next.js + Django + MySQL”,\n**So that** I can have a Next.js SSR React app and a Django backend with a MySQL schema.\n\n**Acceptance Criteria:**\n1. AINative templates exist for:\n   - `django-model`\n   - `django-rest-route` or `drf-viewset`\n   - `next-page` (React + SSR)\n   - `next-api-route`\n   - Manifest files: `frontend/package.json`, `backend/requirements.txt`\n2. `app_generation_plan.txt` handles `tech_stack == \"Next.js + Django + MySQL\"` with a JSON plan containing steps like:\n   ```json\n   {\n     \"tool\": \"codegen_create\",\n     \"input\": { \"template\": \"django-model\", \"file_path\": \"backend/app/models.py\", \"variables\": { ... } }\n   },\n   {\n     \"tool\": \"codegen_create\",\n     \"input\": { \"template\": \"django-rest-route\", \"file_path\": \"backend/app/urls.py\", \"variables\": { ... } }\n   },\n   {\n     \"tool\": \"codegen_create\",\n     \"input\": { \"template\": \"next-page\", \"file_path\": \"frontend/pages/index.jsx\", \"variables\": { ... } }\n   }\n   ```\n3. Frontend `SpecForm.jsx` includes the new option.\n4. `POST /generate-app` with this stack generates a ZIP containing:\n   - `frontend/` with Next.js pages and `package.json` for Next, React, etc.\n   - `backend/` with Django app folder, `models.py`, `views.py`, `urls.py`, `requirements.txt` listing `django`, `djangorestframework`, `mysqlclient`.\n5. Unzipping the ZIP and running `npm install`/`npm run dev` in `frontend` works; running `pip install -r requirements.txt` and `python manage.py runserver` in `backend` works without syntax errors.\n",
    "labels": ["Epic 2", "Sprint 2"]
  },
  {
    "title": "US2.3: Dynamic Prompt Branching Based on Stack",
    "body": "****As a** developer,\n**I want** the planning prompt to dynamically insert the correct set of templates and file paths based on `tech_stack` and `styling`,\n**So that** Ollama always knows which templates to call.\n\n**Acceptance Criteria:**\n1. `app_generation_plan.txt` includes conditional logic (in prompt-language) such as:\n   ```text\n   If tech_stack == \"Vue + Node.js + MongoDB\": use templates [\"mongoose-model\", \"express-route\", \"vue-component\"].\n   If tech_stack == \"Next.js + Django + MySQL\": use templates [\"django-model\", \"django-rest-route\", \"next-page\", \"next-api-route\"].\n   If tech_stack == \"React + FastAPI + PostgreSQL\": use templates [\"sqlalchemy-model\", \"fastapi-route\", \"react-component\"].\n   For any styling == \"TailwindCSS\", add instructions to include Tailwind classes.\n   For any styling == \"Bootstrap\", add instructions to include Bootstrap classes.\n   ```\n2. When testing with a sample spec for each stack, Ollama’s output is valid JSON listing exactly the correct `tool` and `input` for each step, with no mixing of templates across stacks.\n3. A simple automated test script (e.g., using `pytest`) calls Ollama with each stack’s spec and verifies that all `tool` values in the returned plan match the expected set.\n",
    "labels": ["Epic 2", "Sprint 2"]
  },
  {
    "title": "US2.4: Front-End Form Enhancements",
    "body": "****As a** user,\n**I want** additional dropdown options for `tech_stack` and `styling`,\n**So that** I can choose from the expanded set without editing code.\n\n**Acceptance Criteria:**\n- In `SpecForm.jsx`, the `Tech Stack` `<select>` includes at least the following options:\n  - \"React + FastAPI + PostgreSQL\"\n  - \"Vue + Node.js + MongoDB\"\n  - \"Next.js + Django + MySQL\"\n- In `SpecForm.jsx`, the `Styling` `<select>` includes:\n  - \"TailwindCSS\"\n  - \"Bootstrap\"\n  - \"Plain CSS\"\n- When the user selects a new combo, the form’s state updates accordingly.\n- Inspecting the network call (DevTools) confirms the JSON payload includes the newly selected `tech_stack` and `styling` keys.\n",
    "labels": ["Epic 2", "Sprint 2"]
  },
  {
    "title": "US2.5: Test & Validate New Stacks",
    "body": "****As a** QA/dev,\n**I want** to run `/generate-app` for each new stack with a minimal set of features (e.g., only \"User login\"),\n**So that** I can validate the generated scaffolds compile or at least the directories/files exist.\n\n**Acceptance Criteria:**\n1. For each of these stacks: \n   - \"Vue + Node.js + MongoDB\"\n   - \"Next.js + Django + MySQL\"\n   - \"React + FastAPI + PostgreSQL\" (baseline)\n   \n   Run `POST /generate-app` with a spec containing a single feature (e.g., [\"User login\"]).\n2. After receiving the ZIP from `/generate-app`, unzip and verify that:\n   - The expected directory structure exists (e.g., `frontend/src/*.vue` for Vue, `backend/app/models.py` for Django).\n   - Manifest files (e.g., `package.json`, `requirements.txt`) exist.\n3. Attempt a smoke build for each:\n   - `cd frontend && npm install && npm run build` (for Vue/Next/React). Should complete without errors.\n   - `cd backend && pip install -r requirements.txt && <run server>` (for Django/FastAPI), or `npm install && <run server>` (for Node/Express), or `pip install && alembic migrate` (for SQLAlchemy). No syntax errors.\n4. Log any failures as bugs in GitHub with a link to the defect.\n",
    "labels": ["Epic 2", "Sprint 2"]
  },
  {
    "title": "US3.1: Define Agent Roles & Sequences",
    "body": "****As a** architect,\n**I want** to design a \"sequence definition\" JSON that identifies the set of agents (e.g., `DBSchemaAgent`, `BackendAgent`, `FrontendAgent`, `StylingAgent`) and their inter-dependencies,\n**So that** I can submit a single `/agent/coordination/sequences` request to AINative.\n\n**Acceptance Criteria:**\n- A JSON template file `backend/coordination/sequence_template.json` exists with placeholders (e.g., `{project_id}`, `{project_name}`) and fields:\n  ```json\n  {\n    \"agents\": [\"DBSchemaAgent\", \"BackendAgent\", \"FrontendAgent\", \"StylingAgent\"],\n    \"workflow\": [\n      { \"agent\": \"DBSchemaAgent\", \"action\": \"generate_models\", \"depends_on\": [] },\n      { \"agent\": \"BackendAgent\", \"action\": \"generate_routes\", \"depends_on\": [\"DBSchemaAgent\"] },\n      { \"agent\": \"FrontendAgent\", \"action\": \"generate_components\", \"depends_on\": [\"BackendAgent\"] },\n      { \"agent\": \"StylingAgent\", \"action\": \"apply_styles\", \"depends_on\": [\"FrontendAgent\"] }\n    ],\n    \"metadata\": { \"project_id\": \"{project_id}\", \"project_name\": \"{project_name}\" }\n  }\n  ```\n- A markdown file `backend/coordination/README.md` explains: \n  1. The concept of `agents` array and `workflow` steps.\n  2. How to replace placeholders.\n  3. Where to define additional agents in `backend/agents/`.\n- The template is validated to be syntactically correct JSON (no trailing commas, correct quotes).\n",
    "labels": ["Epic 3", "Sprint 3"]
  },
  {
    "title": "US3.2: Submit Coordination Sequence & Polling",
    "body": "****As a** developer,\n**I want** the backend to call `POST /api/v1/agent/coordination/sequences` with the filled-in sequence JSON (replacing placeholders),\n**So that** AINative begins multi-agent execution.\n\n**Acceptance Criteria:**\n1. Implement `tools/coordination_tool.py` with methods:\n   - `create_sequence(sequence_payload: dict) -> { sequence_id: str }` wrapping `POST /api/v1/agent/coordination/sequences`.\n   - `execute_sequence(sequence_id: str) -> {}`, wrapping `POST /api/v1/agent/coordination/sequences/{sequence_id}/execute`.\n   - `get_task_status(task_id: str) -> { status: str, result: dict }`, wrapping `GET /api/v1/agent/coordination/tasks/{task_id}`.\n2. In `/generate-app`, after inserting `generation_steps`, call:\n   ```python\n   seq_payload = fill_sequence_template(project_id, project_name)\n   resp = CoordinationTool().create_sequence(seq_payload)\n   sequence_id = resp[\"sequence_id\"]\n   CoordinationTool().execute_sequence(sequence_id)\n   ```\n3. Implement a poller that periodically (every 2 seconds) calls `get_task_status` for each returned `task_id` until all tasks have `status == \"completed\"` or `\"failed\"`.\n4. Insert logs into `logs` for each task start, completion, and result.\n5. If any task fails, record that in the project’s `status` and include an error log.\n",
    "labels": ["Epic 3", "Sprint 3"]
  },
  {
    "title": "US3.3: Agent-Level Implementations",
    "body": "****As a** developer,\n**I want** to create LangChain \"sub-agents\" that correspond to each role (`DBSchemaAgent`, `BackendAgent`, `FrontendAgent`, `StylingAgent`),\n**So that** when a task is dispatched from AINative, my code picks up the task details and invokes the right AINative tool(s) to generate code.\n\n**Acceptance Criteria:**\n1. Four Python classes exist under `backend/agents/`:\n   - `db_schema_agent.py`\n   - `backend_agent.py`\n   - `frontend_agent.py`\n   - `styling_agent.py`\n2. Each agent class defines a method `run(task_payload: dict)` which:\n   - Extracts `template`, `file_path`, and `variables` from `task_payload`.\n   - Calls the appropriate AINative wrapper (e.g., `CodeGenCreateTool`).\n   - Writes the returned code to disk using `file_writer.write_file` if present.\n   - Returns a result JSON to be sent back to AINative or logged.\n3. Implement a FastAPI route `POST /coord-task-callback` that AINative can call when a task is complete:\n   ```python\n   @app.post(\"/coord-task-callback\")\n   async def coord_task_callback(payload: dict):\n       agent_name = payload[\"agent_id\"]\n       task_input = payload[\"input\"]\n       agent = get_agent_instance(agent_name)\n       result = agent.run(task_input)\n       return {\"status\": \"acknowledged\", \"result\": result}\n   ```\n4. If webhooks are not available, a polling mechanism picks up tasks, identifies `agent_id`, and dispatches to the correct agent class.\n5. Each agent’s `run` method logs its actions to `logs`, e.g., `\"DBSchemaAgent: Generated backend/app/models.py\"`.\n",
    "labels": ["Epic 3", "Sprint 3"]
  },
  {
    "title": "US3.4: Parallel Execution Validation",
    "body": "****As a** QA/dev,\n**I want** to generate a simple spec that yields a 4-step sequence, then run the multi-agent flow and confirm that at least two non-dependent agents run concurrently,\n**So that** I know parallelism is working.\n\n**Acceptance Criteria:**\n1. Create a spec with features requiring:\n   - DB schema generation\n   - Backend routes\n   - Frontend components\n   - Styling\n2. Call `POST /generate-app` with this spec (using `USE_COORDINATION=true`).\n3. In the backend logs (or via database timestamps in `logs`), identify at least two tasks with overlapping \"start_time\" for their respective agents that do not depend on each other (e.g., `DBSchemaAgent` and `FrontendAgent` should start concurrently if they have no direct dependency).\n4. Confirm code files appear on disk for those agents (e.g., `backend/app/models.py` and `frontend/src/components/Login.jsx`) around the same timestamp.\n5. No deadlocks or infinite waits: the entire sequence completes with all tasks reaching `status == \"completed\"`.\n",
    "labels": ["Epic 3", "Sprint 3"]
  },
  {
    "title": "US3.5: Fall-Back to Single-Agent Mode",
    "body": "****As a** developer,\n**I want** a configuration flag (`USE_COORDINATION=false`) that reverts to the old single-loop execution (execute step-by-step sequentially),\n**So that** I can quickly switch back if the Coordination API is down or misbehaving.\n\n**Acceptance Criteria:**\n1. Add an environment variable `USE_COORDINATION` (default set to `true`).\n2. In `backend/main.py` inside the `generate_app` handler, wrap multi-agent logic:\n   ```python\n   if os.getenv(\"USE_COORDINATION\", \"true\").lower() == \"true\":\n       run_multi_agent_flow()\n   else:\n       run_sequential_execution_loop()\n   ```\n3. When `USE_COORDINATION=false`, confirm that code executes the existing sequential loop (calling each AINative tool in order, writing code, zipping) without invoking `coordination_tool` methods.\n4. Write a shell test:\n   ```bash\n   export USE_COORDINATION=false\n   curl -X POST \"http://localhost:8000/generate-app\" -H \"Content-Type: application/json\" -d '{ ... }'\n   ```\n   Observe that no coordination API calls are made and the sequential loop executes.\n",
    "labels": ["Epic 3", "Sprint 3"]
  },
  {
    "title": "US4.1: Drag-and-Drop Component Palette",
    "body": "****As a** user,\n**I want** a \"Component Palette\" sidebar listing \"Login Form,\" \"Notes List,\" \"Note Editor,\" \"Dashboard,\"\n**So that** I can drag a component onto a \"Canvas\" area to visually sketch my app’s UI.\n\n**Acceptance Criteria:**\n1. A suitable drag-and-drop library (e.g., `react-dnd`) is installed and configured in `frontend/`.\n2. Create `ComponentPalette.jsx` listing draggable items:\n   - LoginForm\n   - NotesList\n   - NoteEditor\n   - Dashboard\n   Each item has an icon and a label.\n3. Create a `Canvas.jsx` component that accepts drop events and adds a draggable placeholder widget (e.g., `<LoginWidget />`) at the drop location.\n4. Maintain front-end state `selectedComponents = [{ \"type\": \"LoginForm\", \"x\": 50, \"y\": 80 }, …]`.\n5. CSS ensures widgets are absolutely positioned relative to the canvas container.\n6. Dragging multiple items into the canvas updates the state correctly.\n",
    "labels": ["Epic 4", "Sprint 4"]
  },
  {
    "title": "US4.2: Per-Component Configuration Panel",
    "body": "****As a** user,\n**I want** to click a component in the canvas and see a small modal or side panel where I can rename fields (e.g., “Username” → “Email Address”),\n**So that** I can customize labels and field names before code generation.\n\n**Acceptance Criteria:**\n1. Implement `ComponentConfigModal.jsx` that opens when a canvas widget is clicked.\n2. The modal contains inputs for configurable props, e.g.,\n   - For `LoginForm`: `Username Label`, `Password Label`.\n   - For `NotesList`: `Show Timestamps` (checkbox).\n3. Store these configurations in `state.componentConfigs = { <widgetId>: { usernameLabel: \"User Email\", passwordLabel: \"Pwd\" }, … }`.\n4. Changing values in the modal updates `componentConfigs` in real time.\n5. Closing the modal persists the config for that widget; reopening shows existing config.\n",
    "labels": ["Epic 4", "Sprint 4"]
  },
  {
    "title": "US4.3: Live Preview Rendering",
    "body": "****As a** user,\n**I want** to see a live preview (client-side) of how my combined components will look (HTML/CSS) based on the current canvas layout and configs,\n**So that** I have immediate visual feedback before generating code.\n\n**Acceptance Criteria:**\n1. For each entry in `selectedComponents`, map to a React preview component, e.g., `<LoginPreview config={componentConfigs[id]} />`.\n2. Render all preview components inside a `PreviewContainer` styled as a mobile or desktop viewport.\n3. The preview uses the selected styling framework:\n   - If `styling == \"TailwindCSS\"`, preview components include Tailwind classes.\n   - If `styling == \"Bootstrap\"`, preview components include Bootstrap classes.\n4. Any change to `componentConfigs` or `styling` triggers re-render of the preview.\n5. Preview shows actual CSS styling (not just placeholders), so the user sees a reasonably accurate UI mockup.\n",
    "labels": ["Epic 4", "Sprint 4"]
  },
  {
    "title": "US4.4: Save/Load Canvas Layout",
    "body": "****As a** user,\n**I want** to save my current canvas layout and component configs into `localStorage`,\n**So that** if I accidentally reload or navigate away, I can restore my work.\n\n**Acceptance Criteria:**\n1. On any change to `selectedComponents` or `componentConfigs`, run:\n   ```js\n   localStorage.setItem(\"canvasState\", JSON.stringify({ selectedComponents, componentConfigs }));\n   ```\n2. On component mount (App or Canvas), check `localStorage.canvasState`:\n   - If present, parse and restore `selectedComponents` and `componentConfigs` to state.\n3. Provide a “Reset Canvas” button that:\n   - Clears `selectedComponents` and `componentConfigs`.\n   - Calls `localStorage.removeItem(\"canvasState\")`.\n4. After a page refresh, previously placed widgets and their configs reappear in the canvas.\n",
    "labels": ["Epic 4", "Sprint 4"]
  },
  {
    "title": "US4.5: Integrate CanvasLayout Into Payload",
    "body": "****As a** developer,\n**I want** the final `/generate-app` request to include an additional field `\"canvasLayout\"` (JSON array of component types, positions, and configs),\n**So that** the backend can decide which code templates to invoke for each component.\n\n**Acceptance Criteria:**\n1. Modify the `handleGenerate` function in `App.jsx` so that the JSON payload includes:\n   ```json\n   \"canvasLayout\": [\n     { \"type\": \"LoginForm\", \"x\": 50, \"y\": 80, \"config\": { \"usernameLabel\": \"Email\" } },\n     { \"type\": \"NotesList\", \"x\": 200, \"y\": 80, \"config\": { \"showTimestamps\": true } }\n   ]\n   ```\n2. Alter the `projects` table to add a new column:\n   ```sql\n   ALTER TABLE projects ADD COLUMN canvas_layout JSONB;\n   ```\n3. In `/generate-app`, after validating the base spec, store `canvas_layout` in the new column when inserting `projects`.\n4. Update the planning prompt template (`app_generation_plan.txt`) to include a formatted description of `canvasLayout`, e.g.,:\n   \"Then generate code for components in this order: LoginForm (with Email label), NotesList (show timestamps).\"\n5. Test by sending a payload with `canvasLayout`:\n   - Verify the DB’s `projects.canvas_layout` field contains the correct JSON.\n   - Inspect the planning prompt logged by the backend to confirm it mentions each component and its config.\n",
    "labels": ["Epic 4", "Sprint 4"]
  },
  {
    "title": "US5.1: User Sign-Up / Login",
    "body": "****As a** user,\n**I want** to create an account (via email/password or GitHub OAuth),\n**So that** I can manage my generated apps and have them tied to my identity.\n\n**Acceptance Criteria:**\n1. Create a new `users` table:\n   ```sql\n   CREATE TABLE users (\n     user_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n     email TEXT UNIQUE NOT NULL,\n     name TEXT,\n     password_hash TEXT,\n     github_id TEXT,\n     created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()\n   );\n   ```\n2. Install and configure an authentication library in FastAPI (e.g., `fastapi-users`) or implement custom JWT logic.\n3. Create endpoints:\n   - `POST /auth/register`: accepts `{ \"email\": \"...\", \"password\": \"...\", \"name\": \"...\" }`, hashes the password, inserts into `users`.\n   - `POST /auth/login`: accepts `{ \"email\": \"...\", \"password\": \"...\" }`, verifies credentials, returns a JWT.\n   - `GET /auth/github/login`: redirects to GitHub OAuth.\n   - `GET /auth/github/callback`: receives `code`, exchanges for access token, fetches GitHub user, inserts/updates `users` with `github_id`.\n4. Protect `/generate-app` and `/recall-last-app` with a dependency that verifies the JWT.\n5. If an unauthenticated request hits a protected endpoint, return HTTP 401 Unauthorized.\n",
    "labels": ["Epic 5", "Sprint 5"]
  },
  {
    "title": "US5.2: Associate Projects with Users",
    "body": "****As a** user,\n**I want** my generated apps to appear in a personal Dashboard,\n**So that** I can revisit or regenerate them.\n\n**Acceptance Criteria:**\n1. Alter `projects` table to add `user_id UUID REFERENCES users(user_id)`.\n2. In `/generate-app`, after verifying JWT, extract `user_id` from the token and insert it into the new `projects` row.\n3. Create endpoint `GET /user/projects` that:\n   - Verifies JWT.\n   - Queries `projects` where `user_id = current_user_id`.\n   - Returns `[ { \"project_id\": \"...\", \"project_name\": \"...\", \"status\": \"...\", \"created_at\": \"...\", \"download_url\": \"...\" }, ... ]`.\n4. Frontend:\n   - After login, redirect to `/dashboard`.\n   - On `/dashboard`, fetch `GET /user/projects` and display a table:\n     | Project Name | Status | Created At | Actions         |\n     |--------------|--------|------------|-----------------|\n     | QuickNotes   | SUCCESS| 2025-05-31 | [Download] [Regenerate] |\n   - “Download” links use `download_url`.\n   - “Regenerate” pre-fills form with existing spec (pull from `projects.canvas_layout`).\n",
    "labels": ["Epic 5", "Sprint 5"]
  },
  {
    "title": "US5.3: GitHub Push Integration",
    "body": "****As a** user,\n**I want** to (optionally) push my generated code to a new GitHub repository under my account,\n**So that** I can immediately have a hosted codebase.\n\n**Acceptance Criteria:**\n1. Frontend `SpecForm.jsx` includes a checkbox `\"Push to GitHub\"` visible only if the user is logged in via GitHub OAuth.\n2. On login via GitHub, store the user’s GitHub access token in a secure HTTP-only cookie or session storage.\n3. In `/generate-app`, if `push_to_github = true` in the payload:\n   a. After ZIP creation, call GitHub API:\n      ```http\n      POST https://api.github.com/user/repos\n      Authorization: token <github_access_token>\n      Content-Type: application/json\n      {\n        \"name\": \"<project_name>-<timestamp>\",\n        \"private\": false  \n      }\n      ```\n      → returns `{ \"clone_url\": \"https://github.com/<username>/<repo>.git\" }`.\n   b. On the server, run:\n      ```bash\n      cd temp_projects/<project_id>\n      git init\n      git remote add origin <clone_url>\n      git add .\n      git commit -m \"Initial commit\"\n      git push -u origin main\n      ```\n   c. Capture `github_url = \"https://github.com/<username>/<repo>\"`, update `projects.github_url`.\n4. Return JSON in `/generate-app`: `{ \"status\": \"success\", \"download_url\": \"...\", \"github_url\": \"https://github.com/...\" }`.\n5. Frontend Dashboard shows a “View on GitHub” link if `github_url` is present.\n6. If any GitHub API call fails, record an error in `logs` and return HTTP 500 with a descriptive message.\n",
    "labels": ["Epic 5", "Sprint 5"]
  },
  {
    "title": "US5.4: Email Notifications on Completion",
    "body": "****As a** user,\n**I want** to optionally receive an email when my app generation completes,\n**So that** I don’t have to continuously watch the status panel.\n\n**Acceptance Criteria:**\n1. Frontend `SpecForm.jsx` includes a checkbox `\"Notify me by email when complete\"` if the user has an email on record.\n2. Alter `projects` table: add column `email_notify BOOLEAN DEFAULT FALSE`.\n3. In `/generate-app`, if `email_notify = true`, store `true` in the new column.\n4. After `projects.status` updates to `SUCCESS` or `FAILED`, enqueue an email:\n   - Use Python SMTP (e.g., `smtplib`) or a service like SendGrid.\n   - Send to `users.email` with subject `\"Your App '<project_name>' is Ready\"` and a body containing:\n     ```\n     Hi <user_name>,\n\n     Your app \"<project_name>\" has finished generating.\n\n     • Status: SUCCESS\n     • Download here: http://<host>/downloads/<project_name>.zip\n     • (If GitHub push enabled) View on GitHub: <github_url>\n\n     Thanks for using App Generator!\n     ```\n5. Insert a row into a new table `email_logs (email_log_id UUID, project_id UUID, status TEXT, error_message TEXT, created_at TIMESTAMP WITH TIME ZONE)`.\n6. If email fails (SMTP error), record `status = 'FAILED'` and `error_message` in `email_logs`, and insert a `logs` entry: `\"Email notification failed: <error>\"`.\n",
    "labels": ["Epic 5", "Sprint 5"]
  },
  {
    "title": "US6.1: Dockerize Backend, Frontend & Ollama",
    "body": "****As a** DevOps engineer,\n**I want** to build Docker images for:\n  1. FastAPI backend (with tool wrappers, DB client, Ollama client)\n  2. React frontend (static build)\n  3. Ollama LLM server (using official Ollama Docker image)\n**So that** I can deploy all services consistently in containers.\n\n**Acceptance Criteria:**\n1. Create `backend/Dockerfile`:\n   ```dockerfile\n   FROM python:3.9-slim\n   WORKDIR /app\n   COPY requirements.txt .\n   RUN pip install --no-cache-dir -r requirements.txt\n   COPY . .\n   EXPOSE 8000\n   CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n   ```\n2. Create `frontend/Dockerfile`:\n   ```dockerfile\n   FROM node:16-alpine AS builder\n   WORKDIR /app\n   COPY package.json yarn.lock .\n   RUN yarn install\n   COPY . .\n   RUN yarn build\n\n   FROM nginx:alpine\n   COPY --from=builder /app/build /usr/share/nginx/html\n   EXPOSE 80\n   CMD [\"nginx\", \"-g\", \"daemon off;\"]\n   ```\n3. Create `docker-compose.yml` orchestrating:\n   - `ollama-server` (image: `ollama/ollama:latest`, expose port 11434)\n   - `postgres` (image: `postgres:13-alpine`, expose 5432)\n   - `backend` (build context `./backend`, depends_on `postgres`, expose 8000)\n   - `frontend` (build context `./frontend`, depends_on `backend`, expose 80)\n4. Running `docker-compose up --build` results in:\n   - `http://localhost:11434` responding to Ollama liveness.\n   - `http://localhost:8000/docs` showing FastAPI Swagger UI.\n   - `http://localhost:3000` (or `http://localhost:80` depending on mapping) showing the React UI.\n5. No host Python or Node installation is required; all services run in containers.\n6. Document the Docker setup in the root `README.md`.\n",
    "labels": ["Epic 6", "Sprint 6"]
  },
  {
    "title": "US6.2: Auto-Cleanup of Temporary Projects",
    "body": "****As a** developer,\n**I want** a scheduled job (cron or background worker) that deletes `temp_projects/<project_id>` folders older than 24 hours,\n**So that** disk usage remains under control.\n\n**Acceptance Criteria:**\n1. Create a Python script `backend/cleanup_temp_projects.py`:\n   ```python\n   import os, time, shutil\n\n   BASE = \"temp_projects\"\n   THRESHOLD = 24 * 3600  # seconds in 24 hours\n   now = time.time()\n   for folder in os.listdir(BASE):\n       full_path = os.path.join(BASE, folder)\n       if os.path.isdir(full_path):\n           mtime = os.path.getmtime(full_path)\n           if (now - mtime) > THRESHOLD:\n               shutil.rmtree(full_path)\n               print(f\"Deleted {full_path}\")\n   ```\n2. Schedule this script to run hourly via a cron entry in the host or a separate Docker container:\n   ```cron\n   0 * * * * cd /app/backend && python cleanup_temp_projects.py >> /app/logs/cleanup.log 2>&1\n   ```\n3. In `backend/main.py`, insert a `logs` entry each time a folder is deleted.\n4. Create at least one test directory under `temp_projects/` with `mtime` older than 24 hours, run the script, and confirm it’s deleted.\n5. Confirm directories younger than 24 hours are not removed.\n",
    "labels": ["Epic 6", "Sprint 6"]
  },
  {
    "title": "US6.3: Metrics & Monitoring",
    "body": "****As a** devops/dev,\n**I want** to expose Prometheus-style metrics (e.g., request counts, latencies, Ollama token usage, AINative API call counts),\n**So that** I can build a Grafana dashboard showing system health.\n\n**Acceptance Criteria:**\n1. Install and configure `prometheus_fastapi_instrumentator` in `backend/main.py`:\n   ```python\n   from prometheus_fastapi_instrumentator import Instrumentator\n   instrumentator = Instrumentator()\n   instrumentator.instrument(app).expose(app)\n   ```\n2. Metrics exposed include:\n   - HTTP request count and latency per endpoint (`/generate-app`, `/recall-last-app`, `/auth/*`).\n   - Custom counters for each AINative tool invocation (e.g., `codegen_create_calls_total`, `codegen_refactor_calls_total`).\n   - A gauge for the number of projects with `status == 'IN_PROGRESS'`.\n3. Launch a Prometheus container scraping `http://host.docker.internal:8000/metrics` every 15 seconds.\n4. Create a Grafana dashboard with panels:\n   - Requests/sec by endpoint.\n   - 95th percentile latency for `/generate-app`.\n   - AINative tool calls over time.\n   - Number of in-progress projects.\n5. Verify that metrics update in real time when exercising endpoints.\n6. (Optional) Add an alert rule: if error rate for `/generate-app` > 5% over 5 minutes, trigger a notification.\n",
    "labels": ["Epic 6", "Sprint 6"]
  },
  {
    "title": "US6.4: API Rate Limiting & Throttling",
    "body": "****As a** developer,\n**I want** to enforce a per-user or global rate limit on `POST /generate-app` (e.g., max 5 calls per minute),\n**So that** the system does not get overwhelmed and AINative costs are controlled.\n\n**Acceptance Criteria:**\n1. Install `slowapi` or a similar rate-limiting library in the backend.\n2. Configure a limiter:\n   ```python\n   from slowapi import Limiter\n   from slowapi.util import get_remote_address\n   limiter = Limiter(key_func=get_remote_address)\n   app.state.limiter = limiter\n\n   @app.post(\"/generate-app\")\n   @limiter.limit(\"5/minute\")\n   async def generate_app(...):\n       ...\n   ```\n3. When a single IP (or user) sends 6 rapid `POST /generate-app` requests within 1 minute, the 6th request returns HTTP 429 Too Many Requests with a `Retry-After` header.\n4. Document the policy in the README: “Max 5 generations per minute per IP.”\n5. If rate limit is exceeded, the response body is:\n   ```json\n   { \"detail\": \"Rate limit exceeded: 5 per minute.\" }\n   ```\n",
    "labels": ["Epic 6", "Sprint 6"]
  },
  {
    "title": "US6.5: Performance Tuning & Ollama Resource Management",
    "body": "****As a** devops/LLM engineer,\n**I want** to minimize Ollama’s memory footprint (e.g., swap to `vicuna-7b` for planning) and configure a GPU if available,\n**So that** the service can handle multiple concurrent requests with minimal latency.\n\n**Acceptance Criteria:**\n1. Benchmark `vicuna-13b` vs. `vicuna-7b` on a sample planning prompt (e.g., 100 tokens) and record the 95th percentile latency.\n2. If `vicuna-7b` yields planning < 5 seconds with acceptable plan quality, change the default model in `Ollama(...)` to `vicuna-7b`.\n3. If a GPU is available, confirm Ollama is leveraging GPU by monitoring `nvidia-smi` during planning.\n4. Adjust Ollama parameters: reduce `max_tokens` to the minimum needed (e.g., 512) and set `temperature=0.2` to reduce token usage.\n5. Document new model choice and performance metrics in README.\n6. Run a load test: send 10 concurrent `POST /generate-app` requests; observe 95th percentile planning latency < 8 seconds.\n",
    "labels": ["Epic 6", "Sprint 6"]
  },
  {
    "title": "US7.1: Architecture & Onboarding Guide",
    "body": "****As a** new engineer,\n**I want** a top-level README (or Notion page) that describes:\n  1. High-level data flow (diagram + text).\n  2. How to spin up the stack (prerequisites, Docker Compose, environment variables).\n  3. How to run a basic smoke test.\n**So that** I can get up to speed quickly without tribal knowledge.\n\n**Acceptance Criteria:**\n- README includes a “Quickstart” section:\n  1. `git clone <repo>` && `cd <repo>`\n  2. `docker-compose up --build`\n  3. Visit `http://localhost:3000`\n  4. Fill form, click “Generate App”, download ZIP.\n- Architecture diagram (PNG or ASCII) shows UI → FastAPI → Ollama → AINative → DB.\n- Environment variables documented: `AINATIVE_API_KEY`, `OLLAMA_BASE_URL`, `DATABASE_URL`, `USE_COORDINATION`.\n- Hosted in `README.md` at repo root.\n",
    "labels": ["Epic 7", "Sprint 7"]
  },
  {
    "title": "US7.2: Add-a-New-Stack Tutorial",
    "body": "****As a** new developer,\n**I want** a step-by-step guide on how to add a “Svelte + Go + PostgreSQL” stack,\n**So that** I can replicate the process for any future stack.\n\n**Acceptance Criteria:**\n- Markdown file `docs/adding_new_stack.md` contains:\n  1. Copy-and-paste instructions for AINative template definitions (`svelte-component`, `go-route`, `gorm-model`).\n  2. How to edit `app_generation_plan.txt` to detect `tech_stack == \"Svelte + Go + PostgreSQL\"`.\n  3. How to add new `<option>` to the frontend form for the new stack.\n  4. Example “sample spec” JSON:\n     ```json\n     {\n       \"project_name\": \"SvelteNotes\",\n       \"description\": \"A Svelte + Go notes app\",\n       \"features\": [\"User login\", \"Note CRUD\"],\n       \"tech_stack\": \"Svelte + Go + PostgreSQL\",\n       \"styling\": \"TailwindCSS\",\n       \"canvasLayout\": []\n     }\n     ```\n  5. Expected JSON plan returned by Ollama.  \n- Following the tutorial end-to-end yields a minimal generated project with `frontend/src/*.svelte` and `backend/main.go`.\n",
    "labels": ["Epic 7", "Sprint 7"]
  },
  {
    "title": "US7.3: Multi-Agent Coordination Cookbook",
    "body": "****As a** developer,\n**I want** documentation on how to add a new agent role (e.g., “SecurityAgent” that calls a vulnerability scanner) to the coordination workflow,\n**So that** I can extend the system’s multi-agent capabilities.\n\n**Acceptance Criteria:**\n- Markdown file `docs/coordination_agent_guide.md` includes:\n  1. How to define a new agent in `backend/agents/security_agent.py` with a `run(task_payload)` method.\n  2. How to wrap the AINative tool in `tools/security_scan_tool.py`.\n  3. How to modify `sequence_template.json` to include the new agent and its dependencies, e.g.:\n     ```json\n     {\n       \"agents\": [\"DBSchemaAgent\", \"BackendAgent\", \"SecurityAgent\", \"FrontendAgent\"],\n       \"workflow\": [\n         { \"agent\": \"DBSchemaAgent\", \"action\": \"generate_models\", \"depends_on\": [] },\n         { \"agent\": \"BackendAgent\", \"action\": \"generate_routes\", \"depends_on\": [\"DBSchemaAgent\"] },\n         { \"agent\": \"SecurityAgent\", \"action\": \"scan_security\", \"depends_on\": [\"BackendAgent\"] },\n         { \"agent\": \"FrontendAgent\", \"action\": \"generate_components\", \"depends_on\": [\"SecurityAgent\"] }\n       ],\n       \"metadata\": { \"project_id\": \"{project_id}\", \"project_name\": \"{project_name}\" }\n     }\n     ```\n  4. Example of how to handle a callback to `POST /coord-task-callback` for the new agent.\n- A stub `backend/agents/security_agent.py` is provided with a placeholder `run()` that logs \"Security scan placeholder\".\n",
    "labels": ["Epic 7", "Sprint 7"]
  },
  {
    "title": "US7.4: API Reference & Tool Wrapper Docs",
    "body": "****As a** developer,\n**I want** autogenerated (or manually written) API documentation for all backend endpoints (`/generate-app`, `/recall-last-app`, `/user/projects`, `/auth/*`),\n**So that** I can quickly see request and response schemas.\n\n**Acceptance Criteria:**\n1. FastAPI’s Swagger UI (`/docs`) lists every endpoint, showing:\n   - HTTP method and path.\n   - Request body schema (if POST) with required fields.\n   - Response schemas and status codes.\n2. Each Python file under `backend/tools/` has a README summarizing:\n   - Class name and description.\n   - Method `_call` parameters and expected output.\n3. Export FastAPI’s OpenAPI schema to `docs/openapi.json` using:\n   ```bash\n   curl http://localhost:8000/openapi.json > docs/openapi.json\n   ```\n4. Verify that `openapi.json` accurately reflects all endpoints and models.\n",
    "labels": ["Epic 7", "Sprint 7"]
  }
]
